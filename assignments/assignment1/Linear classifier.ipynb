{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4505/2012955419.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape \n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.006760443547122)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int64)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int32)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float64)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float64)\n",
    "target_index = np.ones(batch_size, dtype=np.int32)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.396315\n",
      "Epoch 1, loss: 2.330164\n",
      "Epoch 2, loss: 2.310063\n",
      "Epoch 3, loss: 2.304436\n",
      "Epoch 4, loss: 2.302607\n",
      "Epoch 5, loss: 2.302139\n",
      "Epoch 6, loss: 2.302193\n",
      "Epoch 7, loss: 2.301429\n",
      "Epoch 8, loss: 2.302836\n",
      "Epoch 9, loss: 2.302241\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7512c1d1b9a0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.117\n",
      "Epoch 0, loss: 2.301632\n",
      "Epoch 1, loss: 2.302215\n",
      "Epoch 2, loss: 2.301335\n",
      "Epoch 3, loss: 2.301710\n",
      "Epoch 4, loss: 2.301328\n",
      "Epoch 5, loss: 2.301327\n",
      "Epoch 6, loss: 2.302049\n",
      "Epoch 7, loss: 2.301469\n",
      "Epoch 8, loss: 2.302031\n",
      "Epoch 9, loss: 2.302234\n",
      "Epoch 10, loss: 2.300800\n",
      "Epoch 11, loss: 2.303007\n",
      "Epoch 12, loss: 2.302145\n",
      "Epoch 13, loss: 2.301248\n",
      "Epoch 14, loss: 2.302280\n",
      "Epoch 15, loss: 2.302009\n",
      "Epoch 16, loss: 2.302015\n",
      "Epoch 17, loss: 2.302057\n",
      "Epoch 18, loss: 2.301948\n",
      "Epoch 19, loss: 2.301786\n",
      "Epoch 20, loss: 2.301964\n",
      "Epoch 21, loss: 2.302204\n",
      "Epoch 22, loss: 2.302030\n",
      "Epoch 23, loss: 2.301058\n",
      "Epoch 24, loss: 2.303381\n",
      "Epoch 25, loss: 2.302251\n",
      "Epoch 26, loss: 2.301575\n",
      "Epoch 27, loss: 2.301162\n",
      "Epoch 28, loss: 2.301526\n",
      "Epoch 29, loss: 2.302203\n",
      "Epoch 30, loss: 2.301968\n",
      "Epoch 31, loss: 2.302026\n",
      "Epoch 32, loss: 2.301708\n",
      "Epoch 33, loss: 2.301696\n",
      "Epoch 34, loss: 2.302655\n",
      "Epoch 35, loss: 2.301769\n",
      "Epoch 36, loss: 2.301931\n",
      "Epoch 37, loss: 2.302218\n",
      "Epoch 38, loss: 2.302306\n",
      "Epoch 39, loss: 2.301360\n",
      "Epoch 40, loss: 2.302042\n",
      "Epoch 41, loss: 2.301683\n",
      "Epoch 42, loss: 2.301885\n",
      "Epoch 43, loss: 2.302580\n",
      "Epoch 44, loss: 2.302574\n",
      "Epoch 45, loss: 2.301861\n",
      "Epoch 46, loss: 2.301477\n",
      "Epoch 47, loss: 2.301888\n",
      "Epoch 48, loss: 2.301374\n",
      "Epoch 49, loss: 2.302225\n",
      "Epoch 50, loss: 2.302380\n",
      "Epoch 51, loss: 2.301898\n",
      "Epoch 52, loss: 2.302504\n",
      "Epoch 53, loss: 2.301637\n",
      "Epoch 54, loss: 2.302142\n",
      "Epoch 55, loss: 2.302661\n",
      "Epoch 56, loss: 2.302774\n",
      "Epoch 57, loss: 2.302709\n",
      "Epoch 58, loss: 2.301824\n",
      "Epoch 59, loss: 2.303111\n",
      "Epoch 60, loss: 2.302491\n",
      "Epoch 61, loss: 2.302605\n",
      "Epoch 62, loss: 2.302084\n",
      "Epoch 63, loss: 2.301318\n",
      "Epoch 64, loss: 2.301441\n",
      "Epoch 65, loss: 2.301920\n",
      "Epoch 66, loss: 2.302013\n",
      "Epoch 67, loss: 2.301714\n",
      "Epoch 68, loss: 2.301642\n",
      "Epoch 69, loss: 2.301550\n",
      "Epoch 70, loss: 2.301826\n",
      "Epoch 71, loss: 2.301495\n",
      "Epoch 72, loss: 2.301492\n",
      "Epoch 73, loss: 2.301306\n",
      "Epoch 74, loss: 2.300974\n",
      "Epoch 75, loss: 2.301135\n",
      "Epoch 76, loss: 2.302394\n",
      "Epoch 77, loss: 2.301846\n",
      "Epoch 78, loss: 2.301285\n",
      "Epoch 79, loss: 2.302031\n",
      "Epoch 80, loss: 2.302611\n",
      "Epoch 81, loss: 2.302736\n",
      "Epoch 82, loss: 2.301978\n",
      "Epoch 83, loss: 2.302458\n",
      "Epoch 84, loss: 2.302987\n",
      "Epoch 85, loss: 2.301565\n",
      "Epoch 86, loss: 2.301576\n",
      "Epoch 87, loss: 2.302051\n",
      "Epoch 88, loss: 2.302691\n",
      "Epoch 89, loss: 2.302824\n",
      "Epoch 90, loss: 2.302185\n",
      "Epoch 91, loss: 2.301815\n",
      "Epoch 92, loss: 2.301662\n",
      "Epoch 93, loss: 2.302055\n",
      "Epoch 94, loss: 2.300915\n",
      "Epoch 95, loss: 2.301983\n",
      "Epoch 96, loss: 2.301396\n",
      "Epoch 97, loss: 2.302153\n",
      "Epoch 98, loss: 2.301778\n",
      "Epoch 99, loss: 2.301925\n",
      "Accuracy after training for 100 epochs:  0.108\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.001, REG=0.0001\n",
      "Epoch 0, loss: 2.300953\n",
      "Epoch 1, loss: 2.301509\n",
      "Epoch 2, loss: 2.301403\n",
      "Epoch 3, loss: 2.297336\n",
      "Epoch 4, loss: 2.301180\n",
      "Epoch 5, loss: 2.300268\n",
      "Epoch 6, loss: 2.297883\n",
      "Epoch 7, loss: 2.292693\n",
      "Epoch 8, loss: 2.295240\n",
      "Epoch 9, loss: 2.294787\n",
      "Epoch 10, loss: 2.291757\n",
      "Epoch 11, loss: 2.296225\n",
      "Epoch 12, loss: 2.291670\n",
      "Epoch 13, loss: 2.290225\n",
      "Epoch 14, loss: 2.289684\n",
      "Epoch 15, loss: 2.287700\n",
      "Epoch 16, loss: 2.289194\n",
      "Epoch 17, loss: 2.284863\n",
      "Epoch 18, loss: 2.282566\n",
      "Epoch 19, loss: 2.285047\n",
      "Epoch 20, loss: 2.283507\n",
      "Epoch 21, loss: 2.283916\n",
      "Epoch 22, loss: 2.285566\n",
      "Epoch 23, loss: 2.282410\n",
      "Epoch 24, loss: 2.281190\n",
      "Epoch 25, loss: 2.272460\n",
      "Epoch 26, loss: 2.281420\n",
      "Epoch 27, loss: 2.269605\n",
      "Epoch 28, loss: 2.282828\n",
      "Epoch 29, loss: 2.282283\n",
      "Epoch 30, loss: 2.274219\n",
      "Epoch 31, loss: 2.271962\n",
      "Epoch 32, loss: 2.274806\n",
      "Epoch 33, loss: 2.271035\n",
      "Epoch 34, loss: 2.275842\n",
      "Epoch 35, loss: 2.279009\n",
      "Epoch 36, loss: 2.267045\n",
      "Epoch 37, loss: 2.269472\n",
      "Epoch 38, loss: 2.268764\n",
      "Epoch 39, loss: 2.273666\n",
      "Epoch 40, loss: 2.279734\n",
      "Epoch 41, loss: 2.259158\n",
      "Epoch 42, loss: 2.266117\n",
      "Epoch 43, loss: 2.275614\n",
      "Epoch 44, loss: 2.272519\n",
      "Epoch 45, loss: 2.274785\n",
      "Epoch 46, loss: 2.260328\n",
      "Epoch 47, loss: 2.267918\n",
      "Epoch 48, loss: 2.273522\n",
      "Epoch 49, loss: 2.260987\n",
      "Epoch 50, loss: 2.267487\n",
      "Epoch 51, loss: 2.250003\n",
      "Epoch 52, loss: 2.261897\n",
      "Epoch 53, loss: 2.257100\n",
      "Epoch 54, loss: 2.261103\n",
      "Epoch 55, loss: 2.266519\n",
      "Epoch 56, loss: 2.264450\n",
      "Epoch 57, loss: 2.257189\n",
      "Epoch 58, loss: 2.258277\n",
      "Epoch 59, loss: 2.255585\n",
      "Epoch 60, loss: 2.252256\n",
      "Epoch 61, loss: 2.247844\n",
      "Epoch 62, loss: 2.241972\n",
      "Epoch 63, loss: 2.255915\n",
      "Epoch 64, loss: 2.254170\n",
      "Epoch 65, loss: 2.251740\n",
      "Epoch 66, loss: 2.251772\n",
      "Epoch 67, loss: 2.262998\n",
      "Epoch 68, loss: 2.251388\n",
      "Epoch 69, loss: 2.232494\n",
      "Epoch 70, loss: 2.266682\n",
      "Epoch 71, loss: 2.250345\n",
      "Epoch 72, loss: 2.255220\n",
      "Epoch 73, loss: 2.261330\n",
      "Epoch 74, loss: 2.259534\n",
      "Epoch 75, loss: 2.259768\n",
      "Epoch 76, loss: 2.255265\n",
      "Epoch 77, loss: 2.255458\n",
      "Epoch 78, loss: 2.250473\n",
      "Epoch 79, loss: 2.229080\n",
      "Epoch 80, loss: 2.246055\n",
      "Epoch 81, loss: 2.261864\n",
      "Epoch 82, loss: 2.248988\n",
      "Epoch 83, loss: 2.233169\n",
      "Epoch 84, loss: 2.235897\n",
      "Epoch 85, loss: 2.255925\n",
      "Epoch 86, loss: 2.251709\n",
      "Epoch 87, loss: 2.232352\n",
      "Epoch 88, loss: 2.238008\n",
      "Epoch 89, loss: 2.238232\n",
      "Epoch 90, loss: 2.246939\n",
      "Epoch 91, loss: 2.247439\n",
      "Epoch 92, loss: 2.246727\n",
      "Epoch 93, loss: 2.231441\n",
      "Epoch 94, loss: 2.248219\n",
      "Epoch 95, loss: 2.237357\n",
      "Epoch 96, loss: 2.229742\n",
      "Epoch 97, loss: 2.250097\n",
      "Epoch 98, loss: 2.217783\n",
      "Epoch 99, loss: 2.254394\n",
      "Epoch 100, loss: 2.243209\n",
      "Epoch 101, loss: 2.236774\n",
      "Epoch 102, loss: 2.233376\n",
      "Epoch 103, loss: 2.235692\n",
      "Epoch 104, loss: 2.237824\n",
      "Epoch 105, loss: 2.221644\n",
      "Epoch 106, loss: 2.241614\n",
      "Epoch 107, loss: 2.234917\n",
      "Epoch 108, loss: 2.231286\n",
      "Epoch 109, loss: 2.243959\n",
      "Epoch 110, loss: 2.246149\n",
      "Epoch 111, loss: 2.236518\n",
      "Epoch 112, loss: 2.222573\n",
      "Epoch 113, loss: 2.236660\n",
      "Epoch 114, loss: 2.241050\n",
      "Epoch 115, loss: 2.239330\n",
      "Epoch 116, loss: 2.216955\n",
      "Epoch 117, loss: 2.207877\n",
      "Epoch 118, loss: 2.230225\n",
      "Epoch 119, loss: 2.256299\n",
      "Epoch 120, loss: 2.207708\n",
      "Epoch 121, loss: 2.224437\n",
      "Epoch 122, loss: 2.244158\n",
      "Epoch 123, loss: 2.229782\n",
      "Epoch 124, loss: 2.232691\n",
      "Epoch 125, loss: 2.201402\n",
      "Epoch 126, loss: 2.240181\n",
      "Epoch 127, loss: 2.226806\n",
      "Epoch 128, loss: 2.221401\n",
      "Epoch 129, loss: 2.248908\n",
      "Epoch 130, loss: 2.236897\n",
      "Epoch 131, loss: 2.218535\n",
      "Epoch 132, loss: 2.218737\n",
      "Epoch 133, loss: 2.212016\n",
      "Epoch 134, loss: 2.239781\n",
      "Epoch 135, loss: 2.237793\n",
      "Epoch 136, loss: 2.229868\n",
      "Epoch 137, loss: 2.220092\n",
      "Epoch 138, loss: 2.239372\n",
      "Epoch 139, loss: 2.225243\n",
      "Epoch 140, loss: 2.215450\n",
      "Epoch 141, loss: 2.229535\n",
      "Epoch 142, loss: 2.237453\n",
      "Epoch 143, loss: 2.228545\n",
      "Epoch 144, loss: 2.212620\n",
      "Epoch 145, loss: 2.210409\n",
      "Epoch 146, loss: 2.236031\n",
      "Epoch 147, loss: 2.200774\n",
      "Epoch 148, loss: 2.227806\n",
      "Epoch 149, loss: 2.230760\n",
      "Epoch 150, loss: 2.218488\n",
      "Epoch 151, loss: 2.216576\n",
      "Epoch 152, loss: 2.215225\n",
      "Epoch 153, loss: 2.217380\n",
      "Epoch 154, loss: 2.197217\n",
      "Epoch 155, loss: 2.233551\n",
      "Epoch 156, loss: 2.212031\n",
      "Epoch 157, loss: 2.224331\n",
      "Epoch 158, loss: 2.182511\n",
      "Epoch 159, loss: 2.226843\n",
      "Epoch 160, loss: 2.228310\n",
      "Epoch 161, loss: 2.211865\n",
      "Epoch 162, loss: 2.200619\n",
      "Epoch 163, loss: 2.205197\n",
      "Epoch 164, loss: 2.229613\n",
      "Epoch 165, loss: 2.243148\n",
      "Epoch 166, loss: 2.215732\n",
      "Epoch 167, loss: 2.204269\n",
      "Epoch 168, loss: 2.197892\n",
      "Epoch 169, loss: 2.229676\n",
      "Epoch 170, loss: 2.216520\n",
      "Epoch 171, loss: 2.210497\n",
      "Epoch 172, loss: 2.207682\n",
      "Epoch 173, loss: 2.203485\n",
      "Epoch 174, loss: 2.203960\n",
      "Epoch 175, loss: 2.193905\n",
      "Epoch 176, loss: 2.231124\n",
      "Epoch 177, loss: 2.204376\n",
      "Epoch 178, loss: 2.212658\n",
      "Epoch 179, loss: 2.207781\n",
      "Epoch 180, loss: 2.218412\n",
      "Epoch 181, loss: 2.207892\n",
      "Epoch 182, loss: 2.206504\n",
      "Epoch 183, loss: 2.219091\n",
      "Epoch 184, loss: 2.202448\n",
      "Epoch 185, loss: 2.182913\n",
      "Epoch 186, loss: 2.213679\n",
      "Epoch 187, loss: 2.197812\n",
      "Epoch 188, loss: 2.222459\n",
      "Epoch 189, loss: 2.223311\n",
      "Epoch 190, loss: 2.192930\n",
      "Epoch 191, loss: 2.199742\n",
      "Epoch 192, loss: 2.190683\n",
      "Epoch 193, loss: 2.181958\n",
      "Epoch 194, loss: 2.190622\n",
      "Epoch 195, loss: 2.230090\n",
      "Epoch 196, loss: 2.166731\n",
      "Epoch 197, loss: 2.222578\n",
      "Epoch 198, loss: 2.197453\n",
      "Epoch 199, loss: 2.207776\n",
      "Val Accuracy: 0.228\n",
      "\n",
      "\n",
      "LR=0.001, REG=1e-05\n",
      "Epoch 0, loss: 2.302024\n",
      "Epoch 1, loss: 2.300068\n",
      "Epoch 2, loss: 2.299063\n",
      "Epoch 3, loss: 2.297579\n",
      "Epoch 4, loss: 2.297428\n",
      "Epoch 5, loss: 2.296233\n",
      "Epoch 6, loss: 2.295413\n",
      "Epoch 7, loss: 2.298311\n",
      "Epoch 8, loss: 2.296751\n",
      "Epoch 9, loss: 2.294041\n",
      "Epoch 10, loss: 2.292089\n",
      "Epoch 11, loss: 2.292508\n",
      "Epoch 12, loss: 2.285730\n",
      "Epoch 13, loss: 2.289031\n",
      "Epoch 14, loss: 2.289792\n",
      "Epoch 15, loss: 2.292103\n",
      "Epoch 16, loss: 2.293121\n",
      "Epoch 17, loss: 2.294123\n",
      "Epoch 18, loss: 2.287077\n",
      "Epoch 19, loss: 2.281569\n",
      "Epoch 20, loss: 2.283713\n",
      "Epoch 21, loss: 2.289463\n",
      "Epoch 22, loss: 2.278758\n",
      "Epoch 23, loss: 2.285420\n",
      "Epoch 24, loss: 2.286099\n",
      "Epoch 25, loss: 2.280102\n",
      "Epoch 26, loss: 2.276671\n",
      "Epoch 27, loss: 2.283750\n",
      "Epoch 28, loss: 2.277208\n",
      "Epoch 29, loss: 2.280195\n",
      "Epoch 30, loss: 2.282022\n",
      "Epoch 31, loss: 2.277434\n",
      "Epoch 32, loss: 2.274425\n",
      "Epoch 33, loss: 2.268154\n",
      "Epoch 34, loss: 2.272207\n",
      "Epoch 35, loss: 2.267517\n",
      "Epoch 36, loss: 2.271713\n",
      "Epoch 37, loss: 2.271124\n",
      "Epoch 38, loss: 2.275538\n",
      "Epoch 39, loss: 2.274827\n",
      "Epoch 40, loss: 2.270821\n",
      "Epoch 41, loss: 2.270706\n",
      "Epoch 42, loss: 2.264848\n",
      "Epoch 43, loss: 2.256069\n",
      "Epoch 44, loss: 2.263867\n",
      "Epoch 45, loss: 2.263871\n",
      "Epoch 46, loss: 2.269814\n",
      "Epoch 47, loss: 2.264008\n",
      "Epoch 48, loss: 2.255533\n",
      "Epoch 49, loss: 2.271053\n",
      "Epoch 50, loss: 2.262233\n",
      "Epoch 51, loss: 2.263808\n",
      "Epoch 52, loss: 2.253542\n",
      "Epoch 53, loss: 2.262621\n",
      "Epoch 54, loss: 2.257809\n",
      "Epoch 55, loss: 2.263949\n",
      "Epoch 56, loss: 2.256151\n",
      "Epoch 57, loss: 2.262881\n",
      "Epoch 58, loss: 2.260919\n",
      "Epoch 59, loss: 2.260926\n",
      "Epoch 60, loss: 2.268043\n",
      "Epoch 61, loss: 2.252107\n",
      "Epoch 62, loss: 2.257706\n",
      "Epoch 63, loss: 2.245847\n",
      "Epoch 64, loss: 2.264184\n",
      "Epoch 65, loss: 2.246472\n",
      "Epoch 66, loss: 2.254358\n",
      "Epoch 67, loss: 2.248783\n",
      "Epoch 68, loss: 2.252335\n",
      "Epoch 69, loss: 2.245060\n",
      "Epoch 70, loss: 2.248541\n",
      "Epoch 71, loss: 2.249753\n",
      "Epoch 72, loss: 2.248185\n",
      "Epoch 73, loss: 2.250757\n",
      "Epoch 74, loss: 2.246714\n",
      "Epoch 75, loss: 2.235388\n",
      "Epoch 76, loss: 2.254226\n",
      "Epoch 77, loss: 2.237129\n",
      "Epoch 78, loss: 2.237827\n",
      "Epoch 79, loss: 2.247273\n",
      "Epoch 80, loss: 2.245453\n",
      "Epoch 81, loss: 2.248534\n",
      "Epoch 82, loss: 2.233950\n",
      "Epoch 83, loss: 2.253590\n",
      "Epoch 84, loss: 2.252116\n",
      "Epoch 85, loss: 2.252798\n",
      "Epoch 86, loss: 2.237917\n",
      "Epoch 87, loss: 2.234770\n",
      "Epoch 88, loss: 2.233850\n",
      "Epoch 89, loss: 2.241251\n",
      "Epoch 90, loss: 2.243371\n",
      "Epoch 91, loss: 2.237035\n",
      "Epoch 92, loss: 2.235149\n",
      "Epoch 93, loss: 2.233595\n",
      "Epoch 94, loss: 2.230973\n",
      "Epoch 95, loss: 2.232443\n",
      "Epoch 96, loss: 2.246707\n",
      "Epoch 97, loss: 2.242028\n",
      "Epoch 98, loss: 2.236836\n",
      "Epoch 99, loss: 2.237289\n",
      "Epoch 100, loss: 2.233744\n",
      "Epoch 101, loss: 2.236038\n",
      "Epoch 102, loss: 2.230117\n",
      "Epoch 103, loss: 2.244294\n",
      "Epoch 104, loss: 2.237145\n",
      "Epoch 105, loss: 2.238923\n",
      "Epoch 106, loss: 2.250167\n",
      "Epoch 107, loss: 2.216468\n",
      "Epoch 108, loss: 2.241197\n",
      "Epoch 109, loss: 2.235522\n",
      "Epoch 110, loss: 2.253831\n",
      "Epoch 111, loss: 2.252221\n",
      "Epoch 112, loss: 2.248654\n",
      "Epoch 113, loss: 2.233880\n",
      "Epoch 114, loss: 2.224641\n",
      "Epoch 115, loss: 2.216768\n",
      "Epoch 116, loss: 2.211362\n",
      "Epoch 117, loss: 2.242460\n",
      "Epoch 118, loss: 2.236085\n",
      "Epoch 119, loss: 2.224966\n",
      "Epoch 120, loss: 2.223789\n",
      "Epoch 121, loss: 2.257726\n",
      "Epoch 122, loss: 2.214722\n",
      "Epoch 123, loss: 2.230459\n",
      "Epoch 124, loss: 2.229979\n",
      "Epoch 125, loss: 2.207912\n",
      "Epoch 126, loss: 2.220779\n",
      "Epoch 127, loss: 2.232447\n",
      "Epoch 128, loss: 2.226366\n",
      "Epoch 129, loss: 2.238486\n",
      "Epoch 130, loss: 2.212852\n",
      "Epoch 131, loss: 2.241980\n",
      "Epoch 132, loss: 2.217768\n",
      "Epoch 133, loss: 2.231968\n",
      "Epoch 134, loss: 2.230877\n",
      "Epoch 135, loss: 2.222865\n",
      "Epoch 136, loss: 2.221806\n",
      "Epoch 137, loss: 2.223000\n",
      "Epoch 138, loss: 2.224682\n",
      "Epoch 139, loss: 2.216961\n",
      "Epoch 140, loss: 2.239026\n",
      "Epoch 141, loss: 2.227076\n",
      "Epoch 142, loss: 2.213472\n",
      "Epoch 143, loss: 2.201448\n",
      "Epoch 144, loss: 2.215733\n",
      "Epoch 145, loss: 2.210369\n",
      "Epoch 146, loss: 2.213144\n",
      "Epoch 147, loss: 2.221451\n",
      "Epoch 148, loss: 2.240672\n",
      "Epoch 149, loss: 2.234980\n",
      "Epoch 150, loss: 2.229384\n",
      "Epoch 151, loss: 2.230410\n",
      "Epoch 152, loss: 2.221122\n",
      "Epoch 153, loss: 2.211862\n",
      "Epoch 154, loss: 2.223709\n",
      "Epoch 155, loss: 2.227460\n",
      "Epoch 156, loss: 2.193444\n",
      "Epoch 157, loss: 2.198531\n",
      "Epoch 158, loss: 2.201269\n",
      "Epoch 159, loss: 2.224548\n",
      "Epoch 160, loss: 2.233503\n",
      "Epoch 161, loss: 2.198606\n",
      "Epoch 162, loss: 2.208010\n",
      "Epoch 163, loss: 2.198863\n",
      "Epoch 164, loss: 2.225298\n",
      "Epoch 165, loss: 2.193669\n",
      "Epoch 166, loss: 2.211926\n",
      "Epoch 167, loss: 2.224866\n",
      "Epoch 168, loss: 2.186165\n",
      "Epoch 169, loss: 2.188906\n",
      "Epoch 170, loss: 2.222730\n",
      "Epoch 171, loss: 2.211925\n",
      "Epoch 172, loss: 2.214156\n",
      "Epoch 173, loss: 2.188228\n",
      "Epoch 174, loss: 2.199202\n",
      "Epoch 175, loss: 2.231655\n",
      "Epoch 176, loss: 2.199786\n",
      "Epoch 177, loss: 2.206785\n",
      "Epoch 178, loss: 2.211723\n",
      "Epoch 179, loss: 2.228334\n",
      "Epoch 180, loss: 2.244953\n",
      "Epoch 181, loss: 2.194470\n",
      "Epoch 182, loss: 2.208003\n",
      "Epoch 183, loss: 2.218910\n",
      "Epoch 184, loss: 2.211883\n",
      "Epoch 185, loss: 2.228477\n",
      "Epoch 186, loss: 2.192075\n",
      "Epoch 187, loss: 2.179675\n",
      "Epoch 188, loss: 2.206886\n",
      "Epoch 189, loss: 2.200664\n",
      "Epoch 190, loss: 2.196423\n",
      "Epoch 191, loss: 2.203657\n",
      "Epoch 192, loss: 2.186247\n",
      "Epoch 193, loss: 2.234393\n",
      "Epoch 194, loss: 2.215309\n",
      "Epoch 195, loss: 2.199166\n",
      "Epoch 196, loss: 2.181656\n",
      "Epoch 197, loss: 2.202627\n",
      "Epoch 198, loss: 2.230758\n",
      "Epoch 199, loss: 2.196379\n",
      "Val Accuracy: 0.227\n",
      "\n",
      "\n",
      "LR=0.001, REG=1e-06\n",
      "Epoch 0, loss: 2.301583\n",
      "Epoch 1, loss: 2.299966\n",
      "Epoch 2, loss: 2.301958\n",
      "Epoch 3, loss: 2.298771\n",
      "Epoch 4, loss: 2.296832\n",
      "Epoch 5, loss: 2.297481\n",
      "Epoch 6, loss: 2.292688\n",
      "Epoch 7, loss: 2.291991\n",
      "Epoch 8, loss: 2.293731\n",
      "Epoch 9, loss: 2.296312\n",
      "Epoch 10, loss: 2.291706\n",
      "Epoch 11, loss: 2.292673\n",
      "Epoch 12, loss: 2.293301\n",
      "Epoch 13, loss: 2.288000\n",
      "Epoch 14, loss: 2.293622\n",
      "Epoch 15, loss: 2.289445\n",
      "Epoch 16, loss: 2.285200\n",
      "Epoch 17, loss: 2.289183\n",
      "Epoch 18, loss: 2.290822\n",
      "Epoch 19, loss: 2.287718\n",
      "Epoch 20, loss: 2.282683\n",
      "Epoch 21, loss: 2.287294\n",
      "Epoch 22, loss: 2.287786\n",
      "Epoch 23, loss: 2.278179\n",
      "Epoch 24, loss: 2.283802\n",
      "Epoch 25, loss: 2.278259\n",
      "Epoch 26, loss: 2.282755\n",
      "Epoch 27, loss: 2.280142\n",
      "Epoch 28, loss: 2.276616\n",
      "Epoch 29, loss: 2.270202\n",
      "Epoch 30, loss: 2.282294\n",
      "Epoch 31, loss: 2.267805\n",
      "Epoch 32, loss: 2.282770\n",
      "Epoch 33, loss: 2.275491\n",
      "Epoch 34, loss: 2.280725\n",
      "Epoch 35, loss: 2.271838\n",
      "Epoch 36, loss: 2.270941\n",
      "Epoch 37, loss: 2.272296\n",
      "Epoch 38, loss: 2.267486\n",
      "Epoch 39, loss: 2.264749\n",
      "Epoch 40, loss: 2.269334\n",
      "Epoch 41, loss: 2.278483\n",
      "Epoch 42, loss: 2.264136\n",
      "Epoch 43, loss: 2.268420\n",
      "Epoch 44, loss: 2.277100\n",
      "Epoch 45, loss: 2.266255\n",
      "Epoch 46, loss: 2.261082\n",
      "Epoch 47, loss: 2.280106\n",
      "Epoch 48, loss: 2.274076\n",
      "Epoch 49, loss: 2.274339\n",
      "Epoch 50, loss: 2.266617\n",
      "Epoch 51, loss: 2.259554\n",
      "Epoch 52, loss: 2.263129\n",
      "Epoch 53, loss: 2.264818\n",
      "Epoch 54, loss: 2.257612\n",
      "Epoch 55, loss: 2.263698\n",
      "Epoch 56, loss: 2.259313\n",
      "Epoch 57, loss: 2.272871\n",
      "Epoch 58, loss: 2.266045\n",
      "Epoch 59, loss: 2.261115\n",
      "Epoch 60, loss: 2.250828\n",
      "Epoch 61, loss: 2.247505\n",
      "Epoch 62, loss: 2.257661\n",
      "Epoch 63, loss: 2.267340\n",
      "Epoch 64, loss: 2.266532\n",
      "Epoch 65, loss: 2.245460\n",
      "Epoch 66, loss: 2.248376\n",
      "Epoch 67, loss: 2.253488\n",
      "Epoch 68, loss: 2.241163\n",
      "Epoch 69, loss: 2.251900\n",
      "Epoch 70, loss: 2.255041\n",
      "Epoch 71, loss: 2.257349\n",
      "Epoch 72, loss: 2.246041\n",
      "Epoch 73, loss: 2.236611\n",
      "Epoch 74, loss: 2.245383\n",
      "Epoch 75, loss: 2.244167\n",
      "Epoch 76, loss: 2.267351\n",
      "Epoch 77, loss: 2.248525\n",
      "Epoch 78, loss: 2.244681\n",
      "Epoch 79, loss: 2.245773\n",
      "Epoch 80, loss: 2.244458\n",
      "Epoch 81, loss: 2.242723\n",
      "Epoch 82, loss: 2.241675\n",
      "Epoch 83, loss: 2.263712\n",
      "Epoch 84, loss: 2.241652\n",
      "Epoch 85, loss: 2.238212\n",
      "Epoch 86, loss: 2.239360\n",
      "Epoch 87, loss: 2.238148\n",
      "Epoch 88, loss: 2.244299\n",
      "Epoch 89, loss: 2.228212\n",
      "Epoch 90, loss: 2.246719\n",
      "Epoch 91, loss: 2.230911\n",
      "Epoch 92, loss: 2.223573\n",
      "Epoch 93, loss: 2.229185\n",
      "Epoch 94, loss: 2.250585\n",
      "Epoch 95, loss: 2.240391\n",
      "Epoch 96, loss: 2.242388\n",
      "Epoch 97, loss: 2.231417\n",
      "Epoch 98, loss: 2.236200\n",
      "Epoch 99, loss: 2.233147\n",
      "Epoch 100, loss: 2.216480\n",
      "Epoch 101, loss: 2.244125\n",
      "Epoch 102, loss: 2.250200\n",
      "Epoch 103, loss: 2.243648\n",
      "Epoch 104, loss: 2.243672\n",
      "Epoch 105, loss: 2.235798\n",
      "Epoch 106, loss: 2.220661\n",
      "Epoch 107, loss: 2.247094\n",
      "Epoch 108, loss: 2.226533\n",
      "Epoch 109, loss: 2.220363\n",
      "Epoch 110, loss: 2.228966\n",
      "Epoch 111, loss: 2.230254\n",
      "Epoch 112, loss: 2.241198\n",
      "Epoch 113, loss: 2.228605\n",
      "Epoch 114, loss: 2.227138\n",
      "Epoch 115, loss: 2.225774\n",
      "Epoch 116, loss: 2.222059\n",
      "Epoch 117, loss: 2.233703\n",
      "Epoch 118, loss: 2.234653\n",
      "Epoch 119, loss: 2.228211\n",
      "Epoch 120, loss: 2.230764\n",
      "Epoch 121, loss: 2.232547\n",
      "Epoch 122, loss: 2.216699\n",
      "Epoch 123, loss: 2.222066\n",
      "Epoch 124, loss: 2.234240\n",
      "Epoch 125, loss: 2.236458\n",
      "Epoch 126, loss: 2.244427\n",
      "Epoch 127, loss: 2.254967\n",
      "Epoch 128, loss: 2.237164\n",
      "Epoch 129, loss: 2.225155\n",
      "Epoch 130, loss: 2.212579\n",
      "Epoch 131, loss: 2.229291\n",
      "Epoch 132, loss: 2.228260\n",
      "Epoch 133, loss: 2.223002\n",
      "Epoch 134, loss: 2.240795\n",
      "Epoch 135, loss: 2.217883\n",
      "Epoch 136, loss: 2.247013\n",
      "Epoch 137, loss: 2.217098\n",
      "Epoch 138, loss: 2.220195\n",
      "Epoch 139, loss: 2.242607\n",
      "Epoch 140, loss: 2.248538\n",
      "Epoch 141, loss: 2.208779\n",
      "Epoch 142, loss: 2.220238\n",
      "Epoch 143, loss: 2.235201\n",
      "Epoch 144, loss: 2.222842\n",
      "Epoch 145, loss: 2.209995\n",
      "Epoch 146, loss: 2.205906\n",
      "Epoch 147, loss: 2.221949\n",
      "Epoch 148, loss: 2.229948\n",
      "Epoch 149, loss: 2.226937\n",
      "Epoch 150, loss: 2.198301\n",
      "Epoch 151, loss: 2.238738\n",
      "Epoch 152, loss: 2.203557\n",
      "Epoch 153, loss: 2.228221\n",
      "Epoch 154, loss: 2.197204\n",
      "Epoch 155, loss: 2.223804\n",
      "Epoch 156, loss: 2.195085\n",
      "Epoch 157, loss: 2.217873\n",
      "Epoch 158, loss: 2.203939\n",
      "Epoch 159, loss: 2.219540\n",
      "Epoch 160, loss: 2.234788\n",
      "Epoch 161, loss: 2.222848\n",
      "Epoch 162, loss: 2.233555\n",
      "Epoch 163, loss: 2.218323\n",
      "Epoch 164, loss: 2.228696\n",
      "Epoch 165, loss: 2.204659\n",
      "Epoch 166, loss: 2.190587\n",
      "Epoch 167, loss: 2.211773\n",
      "Epoch 168, loss: 2.210822\n",
      "Epoch 169, loss: 2.182563\n",
      "Epoch 170, loss: 2.199109\n",
      "Epoch 171, loss: 2.191063\n",
      "Epoch 172, loss: 2.195300\n",
      "Epoch 173, loss: 2.212864\n",
      "Epoch 174, loss: 2.204886\n",
      "Epoch 175, loss: 2.212673\n",
      "Epoch 176, loss: 2.196968\n",
      "Epoch 177, loss: 2.216657\n",
      "Epoch 178, loss: 2.199428\n",
      "Epoch 179, loss: 2.215150\n",
      "Epoch 180, loss: 2.197572\n",
      "Epoch 181, loss: 2.208375\n",
      "Epoch 182, loss: 2.194994\n",
      "Epoch 183, loss: 2.220266\n",
      "Epoch 184, loss: 2.222416\n",
      "Epoch 185, loss: 2.194718\n",
      "Epoch 186, loss: 2.205212\n",
      "Epoch 187, loss: 2.236374\n",
      "Epoch 188, loss: 2.215451\n",
      "Epoch 189, loss: 2.183951\n",
      "Epoch 190, loss: 2.199026\n",
      "Epoch 191, loss: 2.224686\n",
      "Epoch 192, loss: 2.198711\n",
      "Epoch 193, loss: 2.230219\n",
      "Epoch 194, loss: 2.194967\n",
      "Epoch 195, loss: 2.199471\n",
      "Epoch 196, loss: 2.232020\n",
      "Epoch 197, loss: 2.205778\n",
      "Epoch 198, loss: 2.199195\n",
      "Epoch 199, loss: 2.189452\n",
      "Val Accuracy: 0.228\n",
      "\n",
      "\n",
      "LR=0.0001, REG=0.0001\n",
      "Epoch 0, loss: 2.302394\n",
      "Epoch 1, loss: 2.301765\n",
      "Epoch 2, loss: 2.302466\n",
      "Epoch 3, loss: 2.302001\n",
      "Epoch 4, loss: 2.302703\n",
      "Epoch 5, loss: 2.302070\n",
      "Epoch 6, loss: 2.301942\n",
      "Epoch 7, loss: 2.301533\n",
      "Epoch 8, loss: 2.300866\n",
      "Epoch 9, loss: 2.302465\n",
      "Epoch 10, loss: 2.302064\n",
      "Epoch 11, loss: 2.300384\n",
      "Epoch 12, loss: 2.301109\n",
      "Epoch 13, loss: 2.300504\n",
      "Epoch 14, loss: 2.302412\n",
      "Epoch 15, loss: 2.302769\n",
      "Epoch 16, loss: 2.299485\n",
      "Epoch 17, loss: 2.300984\n",
      "Epoch 18, loss: 2.301012\n",
      "Epoch 19, loss: 2.300161\n",
      "Epoch 20, loss: 2.300407\n",
      "Epoch 21, loss: 2.300109\n",
      "Epoch 22, loss: 2.301484\n",
      "Epoch 23, loss: 2.300559\n",
      "Epoch 24, loss: 2.300010\n",
      "Epoch 25, loss: 2.300678\n",
      "Epoch 26, loss: 2.299178\n",
      "Epoch 27, loss: 2.300654\n",
      "Epoch 28, loss: 2.300234\n",
      "Epoch 29, loss: 2.300144\n",
      "Epoch 30, loss: 2.300039\n",
      "Epoch 31, loss: 2.301984\n",
      "Epoch 32, loss: 2.299382\n",
      "Epoch 33, loss: 2.299500\n",
      "Epoch 34, loss: 2.298541\n",
      "Epoch 35, loss: 2.301065\n",
      "Epoch 36, loss: 2.298655\n",
      "Epoch 37, loss: 2.298223\n",
      "Epoch 38, loss: 2.297061\n",
      "Epoch 39, loss: 2.299681\n",
      "Epoch 40, loss: 2.297625\n",
      "Epoch 41, loss: 2.300133\n",
      "Epoch 42, loss: 2.298753\n",
      "Epoch 43, loss: 2.299697\n",
      "Epoch 44, loss: 2.297499\n",
      "Epoch 45, loss: 2.300398\n",
      "Epoch 46, loss: 2.297621\n",
      "Epoch 47, loss: 2.299024\n",
      "Epoch 48, loss: 2.296671\n",
      "Epoch 49, loss: 2.297914\n",
      "Epoch 50, loss: 2.295790\n",
      "Epoch 51, loss: 2.297153\n",
      "Epoch 52, loss: 2.298625\n",
      "Epoch 53, loss: 2.295878\n",
      "Epoch 54, loss: 2.298181\n",
      "Epoch 55, loss: 2.294824\n",
      "Epoch 56, loss: 2.296796\n",
      "Epoch 57, loss: 2.297121\n",
      "Epoch 58, loss: 2.299256\n",
      "Epoch 59, loss: 2.298079\n",
      "Epoch 60, loss: 2.295456\n",
      "Epoch 61, loss: 2.299647\n",
      "Epoch 62, loss: 2.294738\n",
      "Epoch 63, loss: 2.296544\n",
      "Epoch 64, loss: 2.297993\n",
      "Epoch 65, loss: 2.295697\n",
      "Epoch 66, loss: 2.297966\n",
      "Epoch 67, loss: 2.297685\n",
      "Epoch 68, loss: 2.292655\n",
      "Epoch 69, loss: 2.298512\n",
      "Epoch 70, loss: 2.294600\n",
      "Epoch 71, loss: 2.294748\n",
      "Epoch 72, loss: 2.296810\n",
      "Epoch 73, loss: 2.293119\n",
      "Epoch 74, loss: 2.297711\n",
      "Epoch 75, loss: 2.295680\n",
      "Epoch 76, loss: 2.297193\n",
      "Epoch 77, loss: 2.295278\n",
      "Epoch 78, loss: 2.296961\n",
      "Epoch 79, loss: 2.295751\n",
      "Epoch 80, loss: 2.294522\n",
      "Epoch 81, loss: 2.293893\n",
      "Epoch 82, loss: 2.291018\n",
      "Epoch 83, loss: 2.294716\n",
      "Epoch 84, loss: 2.291819\n",
      "Epoch 85, loss: 2.295111\n",
      "Epoch 86, loss: 2.292076\n",
      "Epoch 87, loss: 2.295282\n",
      "Epoch 88, loss: 2.293210\n",
      "Epoch 89, loss: 2.295379\n",
      "Epoch 90, loss: 2.295307\n",
      "Epoch 91, loss: 2.291000\n",
      "Epoch 92, loss: 2.295608\n",
      "Epoch 93, loss: 2.293707\n",
      "Epoch 94, loss: 2.295719\n",
      "Epoch 95, loss: 2.294288\n",
      "Epoch 96, loss: 2.292125\n",
      "Epoch 97, loss: 2.294785\n",
      "Epoch 98, loss: 2.296669\n",
      "Epoch 99, loss: 2.290544\n",
      "Epoch 100, loss: 2.293201\n",
      "Epoch 101, loss: 2.292817\n",
      "Epoch 102, loss: 2.292089\n",
      "Epoch 103, loss: 2.295336\n",
      "Epoch 104, loss: 2.295329\n",
      "Epoch 105, loss: 2.292672\n",
      "Epoch 106, loss: 2.292624\n",
      "Epoch 107, loss: 2.295809\n",
      "Epoch 108, loss: 2.294285\n",
      "Epoch 109, loss: 2.292930\n",
      "Epoch 110, loss: 2.289923\n",
      "Epoch 111, loss: 2.292447\n",
      "Epoch 112, loss: 2.294217\n",
      "Epoch 113, loss: 2.289139\n",
      "Epoch 114, loss: 2.295239\n",
      "Epoch 115, loss: 2.293350\n",
      "Epoch 116, loss: 2.293082\n",
      "Epoch 117, loss: 2.292650\n",
      "Epoch 118, loss: 2.293591\n",
      "Epoch 119, loss: 2.290355\n",
      "Epoch 120, loss: 2.289892\n",
      "Epoch 121, loss: 2.293232\n",
      "Epoch 122, loss: 2.295495\n",
      "Epoch 123, loss: 2.290926\n",
      "Epoch 124, loss: 2.288975\n",
      "Epoch 125, loss: 2.290669\n",
      "Epoch 126, loss: 2.287490\n",
      "Epoch 127, loss: 2.292379\n",
      "Epoch 128, loss: 2.292789\n",
      "Epoch 129, loss: 2.290321\n",
      "Epoch 130, loss: 2.292359\n",
      "Epoch 131, loss: 2.282596\n",
      "Epoch 132, loss: 2.288256\n",
      "Epoch 133, loss: 2.291247\n",
      "Epoch 134, loss: 2.292947\n",
      "Epoch 135, loss: 2.290045\n",
      "Epoch 136, loss: 2.289085\n",
      "Epoch 137, loss: 2.287977\n",
      "Epoch 138, loss: 2.289584\n",
      "Epoch 139, loss: 2.290828\n",
      "Epoch 140, loss: 2.290207\n",
      "Epoch 141, loss: 2.286251\n",
      "Epoch 142, loss: 2.288499\n",
      "Epoch 143, loss: 2.291301\n",
      "Epoch 144, loss: 2.286421\n",
      "Epoch 145, loss: 2.290016\n",
      "Epoch 146, loss: 2.291327\n",
      "Epoch 147, loss: 2.292279\n",
      "Epoch 148, loss: 2.288021\n",
      "Epoch 149, loss: 2.291220\n",
      "Epoch 150, loss: 2.292206\n",
      "Epoch 151, loss: 2.287582\n",
      "Epoch 152, loss: 2.288424\n",
      "Epoch 153, loss: 2.293982\n",
      "Epoch 154, loss: 2.297146\n",
      "Epoch 155, loss: 2.287713\n",
      "Epoch 156, loss: 2.285510\n",
      "Epoch 157, loss: 2.285573\n",
      "Epoch 158, loss: 2.288008\n",
      "Epoch 159, loss: 2.287101\n",
      "Epoch 160, loss: 2.290259\n",
      "Epoch 161, loss: 2.284349\n",
      "Epoch 162, loss: 2.289918\n",
      "Epoch 163, loss: 2.284114\n",
      "Epoch 164, loss: 2.289893\n",
      "Epoch 165, loss: 2.288060\n",
      "Epoch 166, loss: 2.289502\n",
      "Epoch 167, loss: 2.288039\n",
      "Epoch 168, loss: 2.291685\n",
      "Epoch 169, loss: 2.286205\n",
      "Epoch 170, loss: 2.287881\n",
      "Epoch 171, loss: 2.282850\n",
      "Epoch 172, loss: 2.288975\n",
      "Epoch 173, loss: 2.282252\n",
      "Epoch 174, loss: 2.287846\n",
      "Epoch 175, loss: 2.286940\n",
      "Epoch 176, loss: 2.284771\n",
      "Epoch 177, loss: 2.282798\n",
      "Epoch 178, loss: 2.285300\n",
      "Epoch 179, loss: 2.287967\n",
      "Epoch 180, loss: 2.287410\n",
      "Epoch 181, loss: 2.288063\n",
      "Epoch 182, loss: 2.285113\n",
      "Epoch 183, loss: 2.285431\n",
      "Epoch 184, loss: 2.285129\n",
      "Epoch 185, loss: 2.285969\n",
      "Epoch 186, loss: 2.283837\n",
      "Epoch 187, loss: 2.284803\n",
      "Epoch 188, loss: 2.287797\n",
      "Epoch 189, loss: 2.285628\n",
      "Epoch 190, loss: 2.284466\n",
      "Epoch 191, loss: 2.283988\n",
      "Epoch 192, loss: 2.285021\n",
      "Epoch 193, loss: 2.290877\n",
      "Epoch 194, loss: 2.282214\n",
      "Epoch 195, loss: 2.285884\n",
      "Epoch 196, loss: 2.293896\n",
      "Epoch 197, loss: 2.290120\n",
      "Epoch 198, loss: 2.292860\n",
      "Epoch 199, loss: 2.284188\n",
      "Val Accuracy: 0.179\n",
      "\n",
      "\n",
      "LR=0.0001, REG=1e-05\n",
      "Epoch 0, loss: 2.302230\n",
      "Epoch 1, loss: 2.302489\n",
      "Epoch 2, loss: 2.302528\n",
      "Epoch 3, loss: 2.302337\n",
      "Epoch 4, loss: 2.301164\n",
      "Epoch 5, loss: 2.301651\n",
      "Epoch 6, loss: 2.301836\n",
      "Epoch 7, loss: 2.301175\n",
      "Epoch 8, loss: 2.302605\n",
      "Epoch 9, loss: 2.301134\n",
      "Epoch 10, loss: 2.302471\n",
      "Epoch 11, loss: 2.301725\n",
      "Epoch 12, loss: 2.301015\n",
      "Epoch 13, loss: 2.302496\n",
      "Epoch 14, loss: 2.301451\n",
      "Epoch 15, loss: 2.299832\n",
      "Epoch 16, loss: 2.300929\n",
      "Epoch 17, loss: 2.299910\n",
      "Epoch 18, loss: 2.302334\n",
      "Epoch 19, loss: 2.301337\n",
      "Epoch 20, loss: 2.302233\n",
      "Epoch 21, loss: 2.302250\n",
      "Epoch 22, loss: 2.299751\n",
      "Epoch 23, loss: 2.299621\n",
      "Epoch 24, loss: 2.298338\n",
      "Epoch 25, loss: 2.300329\n",
      "Epoch 26, loss: 2.302100\n",
      "Epoch 27, loss: 2.300285\n",
      "Epoch 28, loss: 2.300381\n",
      "Epoch 29, loss: 2.300812\n",
      "Epoch 30, loss: 2.298187\n",
      "Epoch 31, loss: 2.300580\n",
      "Epoch 32, loss: 2.298061\n",
      "Epoch 33, loss: 2.299627\n",
      "Epoch 34, loss: 2.300612\n",
      "Epoch 35, loss: 2.298353\n",
      "Epoch 36, loss: 2.297942\n",
      "Epoch 37, loss: 2.299333\n",
      "Epoch 38, loss: 2.299209\n",
      "Epoch 39, loss: 2.298210\n",
      "Epoch 40, loss: 2.300433\n",
      "Epoch 41, loss: 2.298511\n",
      "Epoch 42, loss: 2.297086\n",
      "Epoch 43, loss: 2.300484\n",
      "Epoch 44, loss: 2.300899\n",
      "Epoch 45, loss: 2.298670\n",
      "Epoch 46, loss: 2.300148\n",
      "Epoch 47, loss: 2.297920\n",
      "Epoch 48, loss: 2.296116\n",
      "Epoch 49, loss: 2.296595\n",
      "Epoch 50, loss: 2.298344\n",
      "Epoch 51, loss: 2.298044\n",
      "Epoch 52, loss: 2.297711\n",
      "Epoch 53, loss: 2.295912\n",
      "Epoch 54, loss: 2.297454\n",
      "Epoch 55, loss: 2.294536\n",
      "Epoch 56, loss: 2.297116\n",
      "Epoch 57, loss: 2.297782\n",
      "Epoch 58, loss: 2.296669\n",
      "Epoch 59, loss: 2.298542\n",
      "Epoch 60, loss: 2.297046\n",
      "Epoch 61, loss: 2.295060\n",
      "Epoch 62, loss: 2.295052\n",
      "Epoch 63, loss: 2.296642\n",
      "Epoch 64, loss: 2.296246\n",
      "Epoch 65, loss: 2.297433\n",
      "Epoch 66, loss: 2.297446\n",
      "Epoch 67, loss: 2.294016\n",
      "Epoch 68, loss: 2.294660\n",
      "Epoch 69, loss: 2.296492\n",
      "Epoch 70, loss: 2.296563\n",
      "Epoch 71, loss: 2.295378\n",
      "Epoch 72, loss: 2.295829\n",
      "Epoch 73, loss: 2.292679\n",
      "Epoch 74, loss: 2.296232\n",
      "Epoch 75, loss: 2.294133\n",
      "Epoch 76, loss: 2.293459\n",
      "Epoch 77, loss: 2.298315\n",
      "Epoch 78, loss: 2.295001\n",
      "Epoch 79, loss: 2.296094\n",
      "Epoch 80, loss: 2.296323\n",
      "Epoch 81, loss: 2.294723\n",
      "Epoch 82, loss: 2.294051\n",
      "Epoch 83, loss: 2.293250\n",
      "Epoch 84, loss: 2.296141\n",
      "Epoch 85, loss: 2.297094\n",
      "Epoch 86, loss: 2.295072\n",
      "Epoch 87, loss: 2.292088\n",
      "Epoch 88, loss: 2.296906\n",
      "Epoch 89, loss: 2.294597\n",
      "Epoch 90, loss: 2.294492\n",
      "Epoch 91, loss: 2.298142\n",
      "Epoch 92, loss: 2.291060\n",
      "Epoch 93, loss: 2.293901\n",
      "Epoch 94, loss: 2.293485\n",
      "Epoch 95, loss: 2.292550\n",
      "Epoch 96, loss: 2.294378\n",
      "Epoch 97, loss: 2.291309\n",
      "Epoch 98, loss: 2.294874\n",
      "Epoch 99, loss: 2.290132\n",
      "Epoch 100, loss: 2.294555\n",
      "Epoch 101, loss: 2.291029\n",
      "Epoch 102, loss: 2.293078\n",
      "Epoch 103, loss: 2.293536\n",
      "Epoch 104, loss: 2.294886\n",
      "Epoch 105, loss: 2.294223\n",
      "Epoch 106, loss: 2.291023\n",
      "Epoch 107, loss: 2.294559\n",
      "Epoch 108, loss: 2.292727\n",
      "Epoch 109, loss: 2.291622\n",
      "Epoch 110, loss: 2.291823\n",
      "Epoch 111, loss: 2.294946\n",
      "Epoch 112, loss: 2.294184\n",
      "Epoch 113, loss: 2.289902\n",
      "Epoch 114, loss: 2.291503\n",
      "Epoch 115, loss: 2.289296\n",
      "Epoch 116, loss: 2.293838\n",
      "Epoch 117, loss: 2.291922\n",
      "Epoch 118, loss: 2.288947\n",
      "Epoch 119, loss: 2.292054\n",
      "Epoch 120, loss: 2.290266\n",
      "Epoch 121, loss: 2.291908\n",
      "Epoch 122, loss: 2.292216\n",
      "Epoch 123, loss: 2.290656\n",
      "Epoch 124, loss: 2.292313\n",
      "Epoch 125, loss: 2.291405\n",
      "Epoch 126, loss: 2.288782\n",
      "Epoch 127, loss: 2.291558\n",
      "Epoch 128, loss: 2.290201\n",
      "Epoch 129, loss: 2.291056\n",
      "Epoch 130, loss: 2.288514\n",
      "Epoch 131, loss: 2.288640\n",
      "Epoch 132, loss: 2.291837\n",
      "Epoch 133, loss: 2.287730\n",
      "Epoch 134, loss: 2.292564\n",
      "Epoch 135, loss: 2.288408\n",
      "Epoch 136, loss: 2.293741\n",
      "Epoch 137, loss: 2.293429\n",
      "Epoch 138, loss: 2.291798\n",
      "Epoch 139, loss: 2.290563\n",
      "Epoch 140, loss: 2.291175\n",
      "Epoch 141, loss: 2.289378\n",
      "Epoch 142, loss: 2.298422\n",
      "Epoch 143, loss: 2.288965\n",
      "Epoch 144, loss: 2.288201\n",
      "Epoch 145, loss: 2.286746\n",
      "Epoch 146, loss: 2.288471\n",
      "Epoch 147, loss: 2.289231\n",
      "Epoch 148, loss: 2.292533\n",
      "Epoch 149, loss: 2.287050\n",
      "Epoch 150, loss: 2.288891\n",
      "Epoch 151, loss: 2.286933\n",
      "Epoch 152, loss: 2.290793\n",
      "Epoch 153, loss: 2.290437\n",
      "Epoch 154, loss: 2.287473\n",
      "Epoch 155, loss: 2.285233\n",
      "Epoch 156, loss: 2.289559\n",
      "Epoch 157, loss: 2.286951\n",
      "Epoch 158, loss: 2.290111\n",
      "Epoch 159, loss: 2.285528\n",
      "Epoch 160, loss: 2.285193\n",
      "Epoch 161, loss: 2.290721\n",
      "Epoch 162, loss: 2.283026\n",
      "Epoch 163, loss: 2.288130\n",
      "Epoch 164, loss: 2.288251\n",
      "Epoch 165, loss: 2.281024\n",
      "Epoch 166, loss: 2.286109\n",
      "Epoch 167, loss: 2.290716\n",
      "Epoch 168, loss: 2.284544\n",
      "Epoch 169, loss: 2.284379\n",
      "Epoch 170, loss: 2.287183\n",
      "Epoch 171, loss: 2.284594\n",
      "Epoch 172, loss: 2.289452\n",
      "Epoch 173, loss: 2.287722\n",
      "Epoch 174, loss: 2.289039\n",
      "Epoch 175, loss: 2.289474\n",
      "Epoch 176, loss: 2.288950\n",
      "Epoch 177, loss: 2.285702\n",
      "Epoch 178, loss: 2.288081\n",
      "Epoch 179, loss: 2.291531\n",
      "Epoch 180, loss: 2.286859\n",
      "Epoch 181, loss: 2.283698\n",
      "Epoch 182, loss: 2.286036\n",
      "Epoch 183, loss: 2.292078\n",
      "Epoch 184, loss: 2.287641\n",
      "Epoch 185, loss: 2.290709\n",
      "Epoch 186, loss: 2.286316\n",
      "Epoch 187, loss: 2.288126\n",
      "Epoch 188, loss: 2.282651\n",
      "Epoch 189, loss: 2.284906\n",
      "Epoch 190, loss: 2.289619\n",
      "Epoch 191, loss: 2.284966\n",
      "Epoch 192, loss: 2.285268\n",
      "Epoch 193, loss: 2.284034\n",
      "Epoch 194, loss: 2.284646\n",
      "Epoch 195, loss: 2.284645\n",
      "Epoch 196, loss: 2.286094\n",
      "Epoch 197, loss: 2.281886\n",
      "Epoch 198, loss: 2.282065\n",
      "Epoch 199, loss: 2.285729\n",
      "Val Accuracy: 0.172\n",
      "\n",
      "\n",
      "LR=0.0001, REG=1e-06\n",
      "Epoch 0, loss: 2.302234\n",
      "Epoch 1, loss: 2.302834\n",
      "Epoch 2, loss: 2.303008\n",
      "Epoch 3, loss: 2.302239\n",
      "Epoch 4, loss: 2.303034\n",
      "Epoch 5, loss: 2.302538\n",
      "Epoch 6, loss: 2.301288\n",
      "Epoch 7, loss: 2.302378\n",
      "Epoch 8, loss: 2.302117\n",
      "Epoch 9, loss: 2.302602\n",
      "Epoch 10, loss: 2.302147\n",
      "Epoch 11, loss: 2.301329\n",
      "Epoch 12, loss: 2.300581\n",
      "Epoch 13, loss: 2.302452\n",
      "Epoch 14, loss: 2.301493\n",
      "Epoch 15, loss: 2.301108\n",
      "Epoch 16, loss: 2.301790\n",
      "Epoch 17, loss: 2.300500\n",
      "Epoch 18, loss: 2.299524\n",
      "Epoch 19, loss: 2.298397\n",
      "Epoch 20, loss: 2.301126\n",
      "Epoch 21, loss: 2.300648\n",
      "Epoch 22, loss: 2.301140\n",
      "Epoch 23, loss: 2.299905\n",
      "Epoch 24, loss: 2.300812\n",
      "Epoch 25, loss: 2.300714\n",
      "Epoch 26, loss: 2.300198\n",
      "Epoch 27, loss: 2.299504\n",
      "Epoch 28, loss: 2.299789\n",
      "Epoch 29, loss: 2.299664\n",
      "Epoch 30, loss: 2.299051\n",
      "Epoch 31, loss: 2.300357\n",
      "Epoch 32, loss: 2.298666\n",
      "Epoch 33, loss: 2.299648\n",
      "Epoch 34, loss: 2.300402\n",
      "Epoch 35, loss: 2.299763\n",
      "Epoch 36, loss: 2.299206\n",
      "Epoch 37, loss: 2.298674\n",
      "Epoch 38, loss: 2.296913\n",
      "Epoch 39, loss: 2.299885\n",
      "Epoch 40, loss: 2.301335\n",
      "Epoch 41, loss: 2.298524\n",
      "Epoch 42, loss: 2.299722\n",
      "Epoch 43, loss: 2.300786\n",
      "Epoch 44, loss: 2.297985\n",
      "Epoch 45, loss: 2.300159\n",
      "Epoch 46, loss: 2.296217\n",
      "Epoch 47, loss: 2.298145\n",
      "Epoch 48, loss: 2.298092\n",
      "Epoch 49, loss: 2.299498\n",
      "Epoch 50, loss: 2.295088\n",
      "Epoch 51, loss: 2.296233\n",
      "Epoch 52, loss: 2.298207\n",
      "Epoch 53, loss: 2.296822\n",
      "Epoch 54, loss: 2.297211\n",
      "Epoch 55, loss: 2.294695\n",
      "Epoch 56, loss: 2.296653\n",
      "Epoch 57, loss: 2.294918\n",
      "Epoch 58, loss: 2.294146\n",
      "Epoch 59, loss: 2.296913\n",
      "Epoch 60, loss: 2.296465\n",
      "Epoch 61, loss: 2.296223\n",
      "Epoch 62, loss: 2.297313\n",
      "Epoch 63, loss: 2.298061\n",
      "Epoch 64, loss: 2.298941\n",
      "Epoch 65, loss: 2.296587\n",
      "Epoch 66, loss: 2.293829\n",
      "Epoch 67, loss: 2.296902\n",
      "Epoch 68, loss: 2.295757\n",
      "Epoch 69, loss: 2.296393\n",
      "Epoch 70, loss: 2.297134\n",
      "Epoch 71, loss: 2.297403\n",
      "Epoch 72, loss: 2.294472\n",
      "Epoch 73, loss: 2.295219\n",
      "Epoch 74, loss: 2.294122\n",
      "Epoch 75, loss: 2.294606\n",
      "Epoch 76, loss: 2.296494\n",
      "Epoch 77, loss: 2.296267\n",
      "Epoch 78, loss: 2.294827\n",
      "Epoch 79, loss: 2.294403\n",
      "Epoch 80, loss: 2.294032\n",
      "Epoch 81, loss: 2.294500\n",
      "Epoch 82, loss: 2.296242\n",
      "Epoch 83, loss: 2.297754\n",
      "Epoch 84, loss: 2.294751\n",
      "Epoch 85, loss: 2.293847\n",
      "Epoch 86, loss: 2.294868\n",
      "Epoch 87, loss: 2.293554\n",
      "Epoch 88, loss: 2.298684\n",
      "Epoch 89, loss: 2.292879\n",
      "Epoch 90, loss: 2.294573\n",
      "Epoch 91, loss: 2.291985\n",
      "Epoch 92, loss: 2.293593\n",
      "Epoch 93, loss: 2.292149\n",
      "Epoch 94, loss: 2.295260\n",
      "Epoch 95, loss: 2.294014\n",
      "Epoch 96, loss: 2.293725\n",
      "Epoch 97, loss: 2.291803\n",
      "Epoch 98, loss: 2.291858\n",
      "Epoch 99, loss: 2.290811\n",
      "Epoch 100, loss: 2.292742\n",
      "Epoch 101, loss: 2.291083\n",
      "Epoch 102, loss: 2.294118\n",
      "Epoch 103, loss: 2.293477\n",
      "Epoch 104, loss: 2.290015\n",
      "Epoch 105, loss: 2.293472\n",
      "Epoch 106, loss: 2.295132\n",
      "Epoch 107, loss: 2.296040\n",
      "Epoch 108, loss: 2.293873\n",
      "Epoch 109, loss: 2.292337\n",
      "Epoch 110, loss: 2.292876\n",
      "Epoch 111, loss: 2.292825\n",
      "Epoch 112, loss: 2.292642\n",
      "Epoch 113, loss: 2.294208\n",
      "Epoch 114, loss: 2.295684\n",
      "Epoch 115, loss: 2.289622\n",
      "Epoch 116, loss: 2.293569\n",
      "Epoch 117, loss: 2.292914\n",
      "Epoch 118, loss: 2.291030\n",
      "Epoch 119, loss: 2.289949\n",
      "Epoch 120, loss: 2.294853\n",
      "Epoch 121, loss: 2.294550\n",
      "Epoch 122, loss: 2.293509\n",
      "Epoch 123, loss: 2.291762\n",
      "Epoch 124, loss: 2.293240\n",
      "Epoch 125, loss: 2.292110\n",
      "Epoch 126, loss: 2.288237\n",
      "Epoch 127, loss: 2.291761\n",
      "Epoch 128, loss: 2.291768\n",
      "Epoch 129, loss: 2.291753\n",
      "Epoch 130, loss: 2.290561\n",
      "Epoch 131, loss: 2.291117\n",
      "Epoch 132, loss: 2.288606\n",
      "Epoch 133, loss: 2.290995\n",
      "Epoch 134, loss: 2.287296\n",
      "Epoch 135, loss: 2.293860\n",
      "Epoch 136, loss: 2.287268\n",
      "Epoch 137, loss: 2.291203\n",
      "Epoch 138, loss: 2.289469\n",
      "Epoch 139, loss: 2.287357\n",
      "Epoch 140, loss: 2.291351\n",
      "Epoch 141, loss: 2.291816\n",
      "Epoch 142, loss: 2.285475\n",
      "Epoch 143, loss: 2.287200\n",
      "Epoch 144, loss: 2.288445\n",
      "Epoch 145, loss: 2.286057\n",
      "Epoch 146, loss: 2.288157\n",
      "Epoch 147, loss: 2.287943\n",
      "Epoch 148, loss: 2.289411\n",
      "Epoch 149, loss: 2.293273\n",
      "Epoch 150, loss: 2.292282\n",
      "Epoch 151, loss: 2.291455\n",
      "Epoch 152, loss: 2.288200\n",
      "Epoch 153, loss: 2.291514\n",
      "Epoch 154, loss: 2.285564\n",
      "Epoch 155, loss: 2.286102\n",
      "Epoch 156, loss: 2.288213\n",
      "Epoch 157, loss: 2.291507\n",
      "Epoch 158, loss: 2.289340\n",
      "Epoch 159, loss: 2.289560\n",
      "Epoch 160, loss: 2.291277\n",
      "Epoch 161, loss: 2.289102\n",
      "Epoch 162, loss: 2.289196\n",
      "Epoch 163, loss: 2.291425\n",
      "Epoch 164, loss: 2.287755\n",
      "Epoch 165, loss: 2.287340\n",
      "Epoch 166, loss: 2.290810\n",
      "Epoch 167, loss: 2.283441\n",
      "Epoch 168, loss: 2.293196\n",
      "Epoch 169, loss: 2.283355\n",
      "Epoch 170, loss: 2.284893\n",
      "Epoch 171, loss: 2.286438\n",
      "Epoch 172, loss: 2.282668\n",
      "Epoch 173, loss: 2.284908\n",
      "Epoch 174, loss: 2.285126\n",
      "Epoch 175, loss: 2.287212\n",
      "Epoch 176, loss: 2.286839\n",
      "Epoch 177, loss: 2.292711\n",
      "Epoch 178, loss: 2.282428\n",
      "Epoch 179, loss: 2.283851\n",
      "Epoch 180, loss: 2.287656\n",
      "Epoch 181, loss: 2.283202\n",
      "Epoch 182, loss: 2.287815\n",
      "Epoch 183, loss: 2.288659\n",
      "Epoch 184, loss: 2.289435\n",
      "Epoch 185, loss: 2.289973\n",
      "Epoch 186, loss: 2.285230\n",
      "Epoch 187, loss: 2.284005\n",
      "Epoch 188, loss: 2.292738\n",
      "Epoch 189, loss: 2.280637\n",
      "Epoch 190, loss: 2.284610\n",
      "Epoch 191, loss: 2.282948\n",
      "Epoch 192, loss: 2.285090\n",
      "Epoch 193, loss: 2.285101\n",
      "Epoch 194, loss: 2.286689\n",
      "Epoch 195, loss: 2.287076\n",
      "Epoch 196, loss: 2.280989\n",
      "Epoch 197, loss: 2.288685\n",
      "Epoch 198, loss: 2.280827\n",
      "Epoch 199, loss: 2.288928\n",
      "Val Accuracy: 0.173\n",
      "\n",
      "\n",
      "LR=1e-05, REG=0.0001\n",
      "Epoch 0, loss: 2.302666\n",
      "Epoch 1, loss: 2.302573\n",
      "Epoch 2, loss: 2.303041\n",
      "Epoch 3, loss: 2.302048\n",
      "Epoch 4, loss: 2.303261\n",
      "Epoch 5, loss: 2.302073\n",
      "Epoch 6, loss: 2.303304\n",
      "Epoch 7, loss: 2.303246\n",
      "Epoch 8, loss: 2.302600\n",
      "Epoch 9, loss: 2.302480\n",
      "Epoch 10, loss: 2.303098\n",
      "Epoch 11, loss: 2.303146\n",
      "Epoch 12, loss: 2.303351\n",
      "Epoch 13, loss: 2.302017\n",
      "Epoch 14, loss: 2.301971\n",
      "Epoch 15, loss: 2.302378\n",
      "Epoch 16, loss: 2.301976\n",
      "Epoch 17, loss: 2.301993\n",
      "Epoch 18, loss: 2.303288\n",
      "Epoch 19, loss: 2.302774\n",
      "Epoch 20, loss: 2.303233\n",
      "Epoch 21, loss: 2.302813\n",
      "Epoch 22, loss: 2.301278\n",
      "Epoch 23, loss: 2.302126\n",
      "Epoch 24, loss: 2.302566\n",
      "Epoch 25, loss: 2.302802\n",
      "Epoch 26, loss: 2.302275\n",
      "Epoch 27, loss: 2.302650\n",
      "Epoch 28, loss: 2.301854\n",
      "Epoch 29, loss: 2.303638\n",
      "Epoch 30, loss: 2.302395\n",
      "Epoch 31, loss: 2.303420\n",
      "Epoch 32, loss: 2.302115\n",
      "Epoch 33, loss: 2.301481\n",
      "Epoch 34, loss: 2.302600\n",
      "Epoch 35, loss: 2.302412\n",
      "Epoch 36, loss: 2.302424\n",
      "Epoch 37, loss: 2.302103\n",
      "Epoch 38, loss: 2.302431\n",
      "Epoch 39, loss: 2.302540\n",
      "Epoch 40, loss: 2.301392\n",
      "Epoch 41, loss: 2.302786\n",
      "Epoch 42, loss: 2.301758\n",
      "Epoch 43, loss: 2.302771\n",
      "Epoch 44, loss: 2.302425\n",
      "Epoch 45, loss: 2.302554\n",
      "Epoch 46, loss: 2.302487\n",
      "Epoch 47, loss: 2.302398\n",
      "Epoch 48, loss: 2.302552\n",
      "Epoch 49, loss: 2.302785\n",
      "Epoch 50, loss: 2.302535\n",
      "Epoch 51, loss: 2.301164\n",
      "Epoch 52, loss: 2.303040\n",
      "Epoch 53, loss: 2.302751\n",
      "Epoch 54, loss: 2.302464\n",
      "Epoch 55, loss: 2.302378\n",
      "Epoch 56, loss: 2.301827\n",
      "Epoch 57, loss: 2.303032\n",
      "Epoch 58, loss: 2.301905\n",
      "Epoch 59, loss: 2.303480\n",
      "Epoch 60, loss: 2.302274\n",
      "Epoch 61, loss: 2.303220\n",
      "Epoch 62, loss: 2.301671\n",
      "Epoch 63, loss: 2.301286\n",
      "Epoch 64, loss: 2.301634\n",
      "Epoch 65, loss: 2.301334\n",
      "Epoch 66, loss: 2.302126\n",
      "Epoch 67, loss: 2.301866\n",
      "Epoch 68, loss: 2.301717\n",
      "Epoch 69, loss: 2.302331\n",
      "Epoch 70, loss: 2.301247\n",
      "Epoch 71, loss: 2.303431\n",
      "Epoch 72, loss: 2.302853\n",
      "Epoch 73, loss: 2.301840\n",
      "Epoch 74, loss: 2.301393\n",
      "Epoch 75, loss: 2.301546\n",
      "Epoch 76, loss: 2.302472\n",
      "Epoch 77, loss: 2.301921\n",
      "Epoch 78, loss: 2.302126\n",
      "Epoch 79, loss: 2.302251\n",
      "Epoch 80, loss: 2.303347\n",
      "Epoch 81, loss: 2.302433\n",
      "Epoch 82, loss: 2.301321\n",
      "Epoch 83, loss: 2.302019\n",
      "Epoch 84, loss: 2.301955\n",
      "Epoch 85, loss: 2.301302\n",
      "Epoch 86, loss: 2.301472\n",
      "Epoch 87, loss: 2.300875\n",
      "Epoch 88, loss: 2.302180\n",
      "Epoch 89, loss: 2.301836\n",
      "Epoch 90, loss: 2.301882\n",
      "Epoch 91, loss: 2.302404\n",
      "Epoch 92, loss: 2.301082\n",
      "Epoch 93, loss: 2.301395\n",
      "Epoch 94, loss: 2.301775\n",
      "Epoch 95, loss: 2.301611\n",
      "Epoch 96, loss: 2.300643\n",
      "Epoch 97, loss: 2.301662\n",
      "Epoch 98, loss: 2.301418\n",
      "Epoch 99, loss: 2.302084\n",
      "Epoch 100, loss: 2.301980\n",
      "Epoch 101, loss: 2.300765\n",
      "Epoch 102, loss: 2.301979\n",
      "Epoch 103, loss: 2.301231\n",
      "Epoch 104, loss: 2.301066\n",
      "Epoch 105, loss: 2.301258\n",
      "Epoch 106, loss: 2.302129\n",
      "Epoch 107, loss: 2.301437\n",
      "Epoch 108, loss: 2.302528\n",
      "Epoch 109, loss: 2.301400\n",
      "Epoch 110, loss: 2.301623\n",
      "Epoch 111, loss: 2.301577\n",
      "Epoch 112, loss: 2.301554\n",
      "Epoch 113, loss: 2.301614\n",
      "Epoch 114, loss: 2.302094\n",
      "Epoch 115, loss: 2.301622\n",
      "Epoch 116, loss: 2.301645\n",
      "Epoch 117, loss: 2.301734\n",
      "Epoch 118, loss: 2.301431\n",
      "Epoch 119, loss: 2.301890\n",
      "Epoch 120, loss: 2.302379\n",
      "Epoch 121, loss: 2.301662\n",
      "Epoch 122, loss: 2.302862\n",
      "Epoch 123, loss: 2.301131\n",
      "Epoch 124, loss: 2.300985\n",
      "Epoch 125, loss: 2.301477\n",
      "Epoch 126, loss: 2.301600\n",
      "Epoch 127, loss: 2.300183\n",
      "Epoch 128, loss: 2.302702\n",
      "Epoch 129, loss: 2.300732\n",
      "Epoch 130, loss: 2.301562\n",
      "Epoch 131, loss: 2.300734\n",
      "Epoch 132, loss: 2.301616\n",
      "Epoch 133, loss: 2.302083\n",
      "Epoch 134, loss: 2.301123\n",
      "Epoch 135, loss: 2.300801\n",
      "Epoch 136, loss: 2.300822\n",
      "Epoch 137, loss: 2.300538\n",
      "Epoch 138, loss: 2.300177\n",
      "Epoch 139, loss: 2.301232\n",
      "Epoch 140, loss: 2.300829\n",
      "Epoch 141, loss: 2.300666\n",
      "Epoch 142, loss: 2.301673\n",
      "Epoch 143, loss: 2.300711\n",
      "Epoch 144, loss: 2.301595\n",
      "Epoch 145, loss: 2.300624\n",
      "Epoch 146, loss: 2.300717\n",
      "Epoch 147, loss: 2.301910\n",
      "Epoch 148, loss: 2.301303\n",
      "Epoch 149, loss: 2.301176\n",
      "Epoch 150, loss: 2.299865\n",
      "Epoch 151, loss: 2.301127\n",
      "Epoch 152, loss: 2.301435\n",
      "Epoch 153, loss: 2.301914\n",
      "Epoch 154, loss: 2.300705\n",
      "Epoch 155, loss: 2.299905\n",
      "Epoch 156, loss: 2.300866\n",
      "Epoch 157, loss: 2.300509\n",
      "Epoch 158, loss: 2.300011\n",
      "Epoch 159, loss: 2.301069\n",
      "Epoch 160, loss: 2.300868\n",
      "Epoch 161, loss: 2.300635\n",
      "Epoch 162, loss: 2.301248\n",
      "Epoch 163, loss: 2.300907\n",
      "Epoch 164, loss: 2.299291\n",
      "Epoch 165, loss: 2.301496\n",
      "Epoch 166, loss: 2.301516\n",
      "Epoch 167, loss: 2.300616\n",
      "Epoch 168, loss: 2.299636\n",
      "Epoch 169, loss: 2.301496\n",
      "Epoch 170, loss: 2.301346\n",
      "Epoch 171, loss: 2.302612\n",
      "Epoch 172, loss: 2.301418\n",
      "Epoch 173, loss: 2.301138\n",
      "Epoch 174, loss: 2.300599\n",
      "Epoch 175, loss: 2.302458\n",
      "Epoch 176, loss: 2.300124\n",
      "Epoch 177, loss: 2.300751\n",
      "Epoch 178, loss: 2.301328\n",
      "Epoch 179, loss: 2.300848\n",
      "Epoch 180, loss: 2.300730\n",
      "Epoch 181, loss: 2.300981\n",
      "Epoch 182, loss: 2.300096\n",
      "Epoch 183, loss: 2.299806\n",
      "Epoch 184, loss: 2.301297\n",
      "Epoch 185, loss: 2.300617\n",
      "Epoch 186, loss: 2.299727\n",
      "Epoch 187, loss: 2.299522\n",
      "Epoch 188, loss: 2.299323\n",
      "Epoch 189, loss: 2.301240\n",
      "Epoch 190, loss: 2.300429\n",
      "Epoch 191, loss: 2.300784\n",
      "Epoch 192, loss: 2.299901\n",
      "Epoch 193, loss: 2.300795\n",
      "Epoch 194, loss: 2.301860\n",
      "Epoch 195, loss: 2.300475\n",
      "Epoch 196, loss: 2.300280\n",
      "Epoch 197, loss: 2.300423\n",
      "Epoch 198, loss: 2.300495\n",
      "Epoch 199, loss: 2.300231\n",
      "Val Accuracy: 0.099\n",
      "\n",
      "\n",
      "LR=1e-05, REG=1e-05\n",
      "Epoch 0, loss: 2.301581\n",
      "Epoch 1, loss: 2.303483\n",
      "Epoch 2, loss: 2.302956\n",
      "Epoch 3, loss: 2.302626\n",
      "Epoch 4, loss: 2.302220\n",
      "Epoch 5, loss: 2.302608\n",
      "Epoch 6, loss: 2.303231\n",
      "Epoch 7, loss: 2.302580\n",
      "Epoch 8, loss: 2.301959\n",
      "Epoch 9, loss: 2.303063\n",
      "Epoch 10, loss: 2.302598\n",
      "Epoch 11, loss: 2.302245\n",
      "Epoch 12, loss: 2.303653\n",
      "Epoch 13, loss: 2.303014\n",
      "Epoch 14, loss: 2.303026\n",
      "Epoch 15, loss: 2.301738\n",
      "Epoch 16, loss: 2.302620\n",
      "Epoch 17, loss: 2.302768\n",
      "Epoch 18, loss: 2.302335\n",
      "Epoch 19, loss: 2.302270\n",
      "Epoch 20, loss: 2.303060\n",
      "Epoch 21, loss: 2.303183\n",
      "Epoch 22, loss: 2.302406\n",
      "Epoch 23, loss: 2.302253\n",
      "Epoch 24, loss: 2.302916\n",
      "Epoch 25, loss: 2.301762\n",
      "Epoch 26, loss: 2.301975\n",
      "Epoch 27, loss: 2.301973\n",
      "Epoch 28, loss: 2.301534\n",
      "Epoch 29, loss: 2.302648\n",
      "Epoch 30, loss: 2.302931\n",
      "Epoch 31, loss: 2.302978\n",
      "Epoch 32, loss: 2.302037\n",
      "Epoch 33, loss: 2.302204\n",
      "Epoch 34, loss: 2.301859\n",
      "Epoch 35, loss: 2.301275\n",
      "Epoch 36, loss: 2.302047\n",
      "Epoch 37, loss: 2.302646\n",
      "Epoch 38, loss: 2.303032\n",
      "Epoch 39, loss: 2.302343\n",
      "Epoch 40, loss: 2.302886\n",
      "Epoch 41, loss: 2.302453\n",
      "Epoch 42, loss: 2.302205\n",
      "Epoch 43, loss: 2.302291\n",
      "Epoch 44, loss: 2.302440\n",
      "Epoch 45, loss: 2.302863\n",
      "Epoch 46, loss: 2.301792\n",
      "Epoch 47, loss: 2.301956\n",
      "Epoch 48, loss: 2.302395\n",
      "Epoch 49, loss: 2.302539\n",
      "Epoch 50, loss: 2.302279\n",
      "Epoch 51, loss: 2.302128\n",
      "Epoch 52, loss: 2.302964\n",
      "Epoch 53, loss: 2.301993\n",
      "Epoch 54, loss: 2.302410\n",
      "Epoch 55, loss: 2.302541\n",
      "Epoch 56, loss: 2.302527\n",
      "Epoch 57, loss: 2.302470\n",
      "Epoch 58, loss: 2.303042\n",
      "Epoch 59, loss: 2.301151\n",
      "Epoch 60, loss: 2.302141\n",
      "Epoch 61, loss: 2.301814\n",
      "Epoch 62, loss: 2.302341\n",
      "Epoch 63, loss: 2.302661\n",
      "Epoch 64, loss: 2.302282\n",
      "Epoch 65, loss: 2.302905\n",
      "Epoch 66, loss: 2.301534\n",
      "Epoch 67, loss: 2.300997\n",
      "Epoch 68, loss: 2.302706\n",
      "Epoch 69, loss: 2.302229\n",
      "Epoch 70, loss: 2.303297\n",
      "Epoch 71, loss: 2.301705\n",
      "Epoch 72, loss: 2.302675\n",
      "Epoch 73, loss: 2.300466\n",
      "Epoch 74, loss: 2.302078\n",
      "Epoch 75, loss: 2.302126\n",
      "Epoch 76, loss: 2.302403\n",
      "Epoch 77, loss: 2.302253\n",
      "Epoch 78, loss: 2.301486\n",
      "Epoch 79, loss: 2.301483\n",
      "Epoch 80, loss: 2.301763\n",
      "Epoch 81, loss: 2.302327\n",
      "Epoch 82, loss: 2.302098\n",
      "Epoch 83, loss: 2.301662\n",
      "Epoch 84, loss: 2.301965\n",
      "Epoch 85, loss: 2.301663\n",
      "Epoch 86, loss: 2.302563\n",
      "Epoch 87, loss: 2.302489\n",
      "Epoch 88, loss: 2.301788\n",
      "Epoch 89, loss: 2.302260\n",
      "Epoch 90, loss: 2.302348\n",
      "Epoch 91, loss: 2.302184\n",
      "Epoch 92, loss: 2.302177\n",
      "Epoch 93, loss: 2.301360\n",
      "Epoch 94, loss: 2.301729\n",
      "Epoch 95, loss: 2.301549\n",
      "Epoch 96, loss: 2.302048\n",
      "Epoch 97, loss: 2.301452\n",
      "Epoch 98, loss: 2.302375\n",
      "Epoch 99, loss: 2.301475\n",
      "Epoch 100, loss: 2.302064\n",
      "Epoch 101, loss: 2.301962\n",
      "Epoch 102, loss: 2.302433\n",
      "Epoch 103, loss: 2.301914\n",
      "Epoch 104, loss: 2.302674\n",
      "Epoch 105, loss: 2.301437\n",
      "Epoch 106, loss: 2.301222\n",
      "Epoch 107, loss: 2.301521\n",
      "Epoch 108, loss: 2.300904\n",
      "Epoch 109, loss: 2.301837\n",
      "Epoch 110, loss: 2.302196\n",
      "Epoch 111, loss: 2.302630\n",
      "Epoch 112, loss: 2.302099\n",
      "Epoch 113, loss: 2.301379\n",
      "Epoch 114, loss: 2.301154\n",
      "Epoch 115, loss: 2.302076\n",
      "Epoch 116, loss: 2.302860\n",
      "Epoch 117, loss: 2.301788\n",
      "Epoch 118, loss: 2.301112\n",
      "Epoch 119, loss: 2.301602\n",
      "Epoch 120, loss: 2.301938\n",
      "Epoch 121, loss: 2.300889\n",
      "Epoch 122, loss: 2.301452\n",
      "Epoch 123, loss: 2.301358\n",
      "Epoch 124, loss: 2.302305\n",
      "Epoch 125, loss: 2.300352\n",
      "Epoch 126, loss: 2.302205\n",
      "Epoch 127, loss: 2.300604\n",
      "Epoch 128, loss: 2.300508\n",
      "Epoch 129, loss: 2.301321\n",
      "Epoch 130, loss: 2.301631\n",
      "Epoch 131, loss: 2.302609\n",
      "Epoch 132, loss: 2.301979\n",
      "Epoch 133, loss: 2.300785\n",
      "Epoch 134, loss: 2.300978\n",
      "Epoch 135, loss: 2.302315\n",
      "Epoch 136, loss: 2.300995\n",
      "Epoch 137, loss: 2.300595\n",
      "Epoch 138, loss: 2.300839\n",
      "Epoch 139, loss: 2.301147\n",
      "Epoch 140, loss: 2.300746\n",
      "Epoch 141, loss: 2.300056\n",
      "Epoch 142, loss: 2.301309\n",
      "Epoch 143, loss: 2.300670\n",
      "Epoch 144, loss: 2.300721\n",
      "Epoch 145, loss: 2.301565\n",
      "Epoch 146, loss: 2.300961\n",
      "Epoch 147, loss: 2.301243\n",
      "Epoch 148, loss: 2.300897\n",
      "Epoch 149, loss: 2.301360\n",
      "Epoch 150, loss: 2.301818\n",
      "Epoch 151, loss: 2.301916\n",
      "Epoch 152, loss: 2.301075\n",
      "Epoch 153, loss: 2.301262\n",
      "Epoch 154, loss: 2.303239\n",
      "Epoch 155, loss: 2.301301\n",
      "Epoch 156, loss: 2.300901\n",
      "Epoch 157, loss: 2.300345\n",
      "Epoch 158, loss: 2.300777\n",
      "Epoch 159, loss: 2.302913\n",
      "Epoch 160, loss: 2.300779\n",
      "Epoch 161, loss: 2.301047\n",
      "Epoch 162, loss: 2.301318\n",
      "Epoch 163, loss: 2.301325\n",
      "Epoch 164, loss: 2.301971\n",
      "Epoch 165, loss: 2.300145\n",
      "Epoch 166, loss: 2.301275\n",
      "Epoch 167, loss: 2.302292\n",
      "Epoch 168, loss: 2.300674\n",
      "Epoch 169, loss: 2.300665\n",
      "Epoch 170, loss: 2.301165\n",
      "Epoch 171, loss: 2.301279\n",
      "Epoch 172, loss: 2.301346\n",
      "Epoch 173, loss: 2.299621\n",
      "Epoch 174, loss: 2.301109\n",
      "Epoch 175, loss: 2.302314\n",
      "Epoch 176, loss: 2.301758\n",
      "Epoch 177, loss: 2.301569\n",
      "Epoch 178, loss: 2.301664\n",
      "Epoch 179, loss: 2.302225\n",
      "Epoch 180, loss: 2.300196\n",
      "Epoch 181, loss: 2.300657\n",
      "Epoch 182, loss: 2.301520\n",
      "Epoch 183, loss: 2.300573\n",
      "Epoch 184, loss: 2.300811\n",
      "Epoch 185, loss: 2.300806\n",
      "Epoch 186, loss: 2.299915\n",
      "Epoch 187, loss: 2.299604\n",
      "Epoch 188, loss: 2.301204\n",
      "Epoch 189, loss: 2.299937\n",
      "Epoch 190, loss: 2.300336\n",
      "Epoch 191, loss: 2.301065\n",
      "Epoch 192, loss: 2.300543\n",
      "Epoch 193, loss: 2.301011\n",
      "Epoch 194, loss: 2.300703\n",
      "Epoch 195, loss: 2.300879\n",
      "Epoch 196, loss: 2.300642\n",
      "Epoch 197, loss: 2.300750\n",
      "Epoch 198, loss: 2.300679\n",
      "Epoch 199, loss: 2.300550\n",
      "Val Accuracy: 0.127\n",
      "\n",
      "\n",
      "LR=1e-05, REG=1e-06\n",
      "Epoch 0, loss: 2.302208\n",
      "Epoch 1, loss: 2.302261\n",
      "Epoch 2, loss: 2.302912\n",
      "Epoch 3, loss: 2.301931\n",
      "Epoch 4, loss: 2.301499\n",
      "Epoch 5, loss: 2.302787\n",
      "Epoch 6, loss: 2.302705\n",
      "Epoch 7, loss: 2.302160\n",
      "Epoch 8, loss: 2.302095\n",
      "Epoch 9, loss: 2.302538\n",
      "Epoch 10, loss: 2.302145\n",
      "Epoch 11, loss: 2.302457\n",
      "Epoch 12, loss: 2.302109\n",
      "Epoch 13, loss: 2.302870\n",
      "Epoch 14, loss: 2.302284\n",
      "Epoch 15, loss: 2.302936\n",
      "Epoch 16, loss: 2.302857\n",
      "Epoch 17, loss: 2.302091\n",
      "Epoch 18, loss: 2.302293\n",
      "Epoch 19, loss: 2.302054\n",
      "Epoch 20, loss: 2.302845\n",
      "Epoch 21, loss: 2.302887\n",
      "Epoch 22, loss: 2.302627\n",
      "Epoch 23, loss: 2.301606\n",
      "Epoch 24, loss: 2.302467\n",
      "Epoch 25, loss: 2.302259\n",
      "Epoch 26, loss: 2.302554\n",
      "Epoch 27, loss: 2.302344\n",
      "Epoch 28, loss: 2.302166\n",
      "Epoch 29, loss: 2.302137\n",
      "Epoch 30, loss: 2.302533\n",
      "Epoch 31, loss: 2.302834\n",
      "Epoch 32, loss: 2.302314\n",
      "Epoch 33, loss: 2.302804\n",
      "Epoch 34, loss: 2.301506\n",
      "Epoch 35, loss: 2.302257\n",
      "Epoch 36, loss: 2.301293\n",
      "Epoch 37, loss: 2.301535\n",
      "Epoch 38, loss: 2.302078\n",
      "Epoch 39, loss: 2.302403\n",
      "Epoch 40, loss: 2.302946\n",
      "Epoch 41, loss: 2.301365\n",
      "Epoch 42, loss: 2.302314\n",
      "Epoch 43, loss: 2.302700\n",
      "Epoch 44, loss: 2.301869\n",
      "Epoch 45, loss: 2.300169\n",
      "Epoch 46, loss: 2.302177\n",
      "Epoch 47, loss: 2.301728\n",
      "Epoch 48, loss: 2.301818\n",
      "Epoch 49, loss: 2.302143\n",
      "Epoch 50, loss: 2.301822\n",
      "Epoch 51, loss: 2.302629\n",
      "Epoch 52, loss: 2.301115\n",
      "Epoch 53, loss: 2.301988\n",
      "Epoch 54, loss: 2.302705\n",
      "Epoch 55, loss: 2.301359\n",
      "Epoch 56, loss: 2.301994\n",
      "Epoch 57, loss: 2.303450\n",
      "Epoch 58, loss: 2.301153\n",
      "Epoch 59, loss: 2.303080\n",
      "Epoch 60, loss: 2.301579\n",
      "Epoch 61, loss: 2.303000\n",
      "Epoch 62, loss: 2.301308\n",
      "Epoch 63, loss: 2.302173\n",
      "Epoch 64, loss: 2.301642\n",
      "Epoch 65, loss: 2.301850\n",
      "Epoch 66, loss: 2.302414\n",
      "Epoch 67, loss: 2.301836\n",
      "Epoch 68, loss: 2.301574\n",
      "Epoch 69, loss: 2.301844\n",
      "Epoch 70, loss: 2.302228\n",
      "Epoch 71, loss: 2.302077\n",
      "Epoch 72, loss: 2.302763\n",
      "Epoch 73, loss: 2.301259\n",
      "Epoch 74, loss: 2.300539\n",
      "Epoch 75, loss: 2.303711\n",
      "Epoch 76, loss: 2.302297\n",
      "Epoch 77, loss: 2.301474\n",
      "Epoch 78, loss: 2.302449\n",
      "Epoch 79, loss: 2.302660\n",
      "Epoch 80, loss: 2.301856\n",
      "Epoch 81, loss: 2.302667\n",
      "Epoch 82, loss: 2.301859\n",
      "Epoch 83, loss: 2.302515\n",
      "Epoch 84, loss: 2.301478\n",
      "Epoch 85, loss: 2.302636\n",
      "Epoch 86, loss: 2.300159\n",
      "Epoch 87, loss: 2.300736\n",
      "Epoch 88, loss: 2.300556\n",
      "Epoch 89, loss: 2.302193\n",
      "Epoch 90, loss: 2.301252\n",
      "Epoch 91, loss: 2.301858\n",
      "Epoch 92, loss: 2.302926\n",
      "Epoch 93, loss: 2.300190\n",
      "Epoch 94, loss: 2.300382\n",
      "Epoch 95, loss: 2.302503\n",
      "Epoch 96, loss: 2.300752\n",
      "Epoch 97, loss: 2.301317\n",
      "Epoch 98, loss: 2.299804\n",
      "Epoch 99, loss: 2.301629\n",
      "Epoch 100, loss: 2.301471\n",
      "Epoch 101, loss: 2.300081\n",
      "Epoch 102, loss: 2.301654\n",
      "Epoch 103, loss: 2.302473\n",
      "Epoch 104, loss: 2.301495\n",
      "Epoch 105, loss: 2.301354\n",
      "Epoch 106, loss: 2.300090\n",
      "Epoch 107, loss: 2.301511\n",
      "Epoch 108, loss: 2.300620\n",
      "Epoch 109, loss: 2.301332\n",
      "Epoch 110, loss: 2.301376\n",
      "Epoch 111, loss: 2.300436\n",
      "Epoch 112, loss: 2.300997\n",
      "Epoch 113, loss: 2.300711\n",
      "Epoch 114, loss: 2.300782\n",
      "Epoch 115, loss: 2.301503\n",
      "Epoch 116, loss: 2.301488\n",
      "Epoch 117, loss: 2.300999\n",
      "Epoch 118, loss: 2.301797\n",
      "Epoch 119, loss: 2.300516\n",
      "Epoch 120, loss: 2.301609\n",
      "Epoch 121, loss: 2.301712\n",
      "Epoch 122, loss: 2.301238\n",
      "Epoch 123, loss: 2.301408\n",
      "Epoch 124, loss: 2.299898\n",
      "Epoch 125, loss: 2.301305\n",
      "Epoch 126, loss: 2.301267\n",
      "Epoch 127, loss: 2.302550\n",
      "Epoch 128, loss: 2.300707\n",
      "Epoch 129, loss: 2.301145\n",
      "Epoch 130, loss: 2.300944\n",
      "Epoch 131, loss: 2.300664\n",
      "Epoch 132, loss: 2.300012\n",
      "Epoch 133, loss: 2.302331\n",
      "Epoch 134, loss: 2.300990\n",
      "Epoch 135, loss: 2.300864\n",
      "Epoch 136, loss: 2.300639\n",
      "Epoch 137, loss: 2.302124\n",
      "Epoch 138, loss: 2.301529\n",
      "Epoch 139, loss: 2.301389\n",
      "Epoch 140, loss: 2.300182\n",
      "Epoch 141, loss: 2.301047\n",
      "Epoch 142, loss: 2.301178\n",
      "Epoch 143, loss: 2.302354\n",
      "Epoch 144, loss: 2.299889\n",
      "Epoch 145, loss: 2.300750\n",
      "Epoch 146, loss: 2.300041\n",
      "Epoch 147, loss: 2.299650\n",
      "Epoch 148, loss: 2.301234\n",
      "Epoch 149, loss: 2.299812\n",
      "Epoch 150, loss: 2.301359\n",
      "Epoch 151, loss: 2.302494\n",
      "Epoch 152, loss: 2.300072\n",
      "Epoch 153, loss: 2.299936\n",
      "Epoch 154, loss: 2.301487\n",
      "Epoch 155, loss: 2.300077\n",
      "Epoch 156, loss: 2.301192\n",
      "Epoch 157, loss: 2.300308\n",
      "Epoch 158, loss: 2.300959\n",
      "Epoch 159, loss: 2.301254\n",
      "Epoch 160, loss: 2.301135\n",
      "Epoch 161, loss: 2.300765\n",
      "Epoch 162, loss: 2.299270\n",
      "Epoch 163, loss: 2.301064\n",
      "Epoch 164, loss: 2.300272\n",
      "Epoch 165, loss: 2.301767\n",
      "Epoch 166, loss: 2.300840\n",
      "Epoch 167, loss: 2.300952\n",
      "Epoch 168, loss: 2.300342\n",
      "Epoch 169, loss: 2.299274\n",
      "Epoch 170, loss: 2.300969\n",
      "Epoch 171, loss: 2.300350\n",
      "Epoch 172, loss: 2.300705\n",
      "Epoch 173, loss: 2.300180\n",
      "Epoch 174, loss: 2.301707\n",
      "Epoch 175, loss: 2.300173\n",
      "Epoch 176, loss: 2.300391\n",
      "Epoch 177, loss: 2.299846\n",
      "Epoch 178, loss: 2.299829\n",
      "Epoch 179, loss: 2.299919\n",
      "Epoch 180, loss: 2.300435\n",
      "Epoch 181, loss: 2.299733\n",
      "Epoch 182, loss: 2.300966\n",
      "Epoch 183, loss: 2.301746\n",
      "Epoch 184, loss: 2.300361\n",
      "Epoch 185, loss: 2.300295\n",
      "Epoch 186, loss: 2.300993\n",
      "Epoch 187, loss: 2.300282\n",
      "Epoch 188, loss: 2.299592\n",
      "Epoch 189, loss: 2.300130\n",
      "Epoch 190, loss: 2.300354\n",
      "Epoch 191, loss: 2.300773\n",
      "Epoch 192, loss: 2.299488\n",
      "Epoch 193, loss: 2.299809\n",
      "Epoch 194, loss: 2.301229\n",
      "Epoch 195, loss: 2.301421\n",
      "Epoch 196, loss: 2.300428\n",
      "Epoch 197, loss: 2.300680\n",
      "Epoch 198, loss: 2.300380\n",
      "Epoch 199, loss: 2.299036\n",
      "Val Accuracy: 0.137\n",
      "\n",
      "\n",
      "best validation accuracy achieved: 0.228000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        print(\"train with learning_rates: {}, reg_strengths: {}\".format(lr, reg))\n",
    "\n",
    "        clf = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = clf.fit(\n",
    "                X=train_X,\n",
    "                y=train_y, \n",
    "                batch_size=batch_size, \n",
    "                learning_rate=lr, \n",
    "                reg=reg, \n",
    "                epochs=num_epochs\n",
    "            )\n",
    "\n",
    "        pred = clf.predict(val_X)\n",
    "        val_accuracy = multiclass_accuracy(pred, val_y)\n",
    "\n",
    "        print(f'Val Accuracy: {val_accuracy}\\n\\n')\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_classifier = clf\n",
    "\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.194000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
